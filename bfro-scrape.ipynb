{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "388c847a",
   "metadata": {},
   "source": [
    "# BFRO Site Scraper\n",
    "\n",
    "This notebook is for scraping Bigfoot sighting data from Bigfoot Field Research Organization's report page HTML found here http://www.bfro.net/gdb/. This involves first requesting the HTML from the '/gdb' page that is the main page for the database of records, and pulling all anchor tags with an href. For Canada this page has the links for provinces and for the US links for every state except Hawaii. We have to then loop through all the state links and get links for each county in each state, then loop through each county to pull out all report links. For Canada the report links are right on the provence pages and are not broken down further. Canadian and US report links are combined, then we loop through and request the page for each report and pull out the data fields from the HTML.\n",
    "Data is pulled from every report and stored in a list of dictionaries which is converted to a Pandas DataFrame then stored as a CSV file for further cleaning and EDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "481d58a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "ed14ecb2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "base_url = \"http://www.bfro.net\"\n",
    "\n",
    "# Request the bfro geographical database page\n",
    "response = requests.get(base_url +\"/gdb\")\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Grab all href elements\n",
    "links = soup.find_all('a', href=True)\n",
    "\n",
    "# Grab state and providence links for US and Canada.\n",
    "# These are stored separately as the US links will require \n",
    "# one more round of scraping than Canada\n",
    "canada_links = []\n",
    "us_state_links = []\n",
    "\n",
    "for a in links:\n",
    "    if \"state=ca-\" in a['href']:\n",
    "        canada_links.append(base_url + a['href'])\n",
    "    elif \"state=int\" in a['href']:\n",
    "        pass\n",
    "    elif \"state\" in a['href']:\n",
    "        us_state_links.append(base_url + a['href'])\n",
    "\n",
    "assert len(us_state_links) == 49\n",
    "assert len(canada_links) == 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "9099a068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through US state links and grab all the county links\n",
    "us_county_links = []\n",
    "\n",
    "for url in us_state_links:\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    assert response.ok\n",
    "    \n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    anchor_tags = soup.find_all('a', href=lambda href: href and \"county\" in href)\n",
    "    \n",
    "    if anchor_tags:\n",
    "        for a in anchor_tags:\n",
    "            us_county_links.append(\"http://www.bfro.net/gdb/\" + a['href'])\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "203d9090",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull out links for the report pages\n",
    "def get_report_urls(urls):\n",
    "    report_urls = []\n",
    "    for url in urls:\n",
    "        response = requests.get(url)\n",
    "        assert response.ok\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        anchor_tags = soup.find_all('a', href=lambda href: href and 'show_report.asp?id' in href)\n",
    "    \n",
    "        if anchor_tags:\n",
    "            for a in anchor_tags:\n",
    "                report_urls.append(\"http://www.bfro.net/gdb/\" + a['href'])\n",
    "    return report_urls\n",
    "\n",
    "# Combine all US county links with the Canadian links.\n",
    "# This leaves a list of all urls for every report on the bfro website\n",
    "report_urls = get_report_urls(us_county_links) + get_report_urls(canada_links)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "id": "89f53320",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5345"
      ]
     },
     "execution_count": 424,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(report_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "id": "e96b50ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_report_data(url):\n",
    "    \n",
    "    report_dict = {}\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Extract the header information stored in span elements with class  \n",
    "    # of either reportheader or reportclassification.\n",
    "    html_class = ['reportheader', 'reportclassification']\n",
    "    for c in html_class:\n",
    "        element = soup.find('span', {'class': c})\n",
    "        if element:\n",
    "            report_dict[c] = element.text.strip()\n",
    "        else:\n",
    "            report_dict[c] = \"did not find \"\n",
    "        \n",
    "    # Extract other details\n",
    "    def extract(text):\n",
    "        if text == \"LOCATION DETAILS\":\n",
    "            element = soup.find('span', {'class':'field'}, string=text)\n",
    "            if element:\n",
    "                return element.parent.text.strip()\n",
    "            else: \n",
    "                return \"did not find\"\n",
    "        else:\n",
    "            element = soup.find('span', {'class': 'field'}, string=text)\n",
    "            if element:\n",
    "                return element.parent.text.replace(text, \"\").strip()\n",
    "            else:\n",
    "                return \"\"\n",
    "\n",
    "    year = extract('YEAR:')\n",
    "    season = extract('SEASON:')\n",
    "    month = extract('MONTH:')\n",
    "    state = extract(\"STATE:\")\n",
    "    county = extract(\"COUNTY:\")\n",
    "    nearest_town = extract(\"NEAREST TOWN:\")\n",
    "    observed = extract(\"OBSERVED:\")\n",
    "    also_noticed = extract(\"ALSO NOTICED:\")\n",
    "    other_witnesses = extract(\"OTHER WITNESSES:\")\n",
    "    other_stories = extract(\"OTHER STORIES:\")\n",
    "    time_and_conditions = extract(\"TIME AND CONDITIONS:\")\n",
    "    environment = extract(\"ENVIRONMENT:\")\n",
    "    country = extract(\"COUNTRY:\")\n",
    "    province = extract(\"PROVINCE:\")\n",
    "    location_details = extract(\"LOCATION DETAILS:\")\n",
    "    \n",
    "    report_dict['year'] = year\n",
    "    report_dict['season'] = season\n",
    "    report_dict['month'] = month\n",
    "    report_dict['state'] = state\n",
    "    report_dict['county'] = county\n",
    "    report_dict['nearest_town'] = nearest_town\n",
    "    report_dict['observed'] = observed\n",
    "    report_dict['also_noticed'] = also_noticed\n",
    "    report_dict['other_witnesses'] = other_witnesses\n",
    "    report_dict['other_stories'] = other_stories\n",
    "    report_dict['time_and_conditions'] = time_and_conditions\n",
    "    report_dict['environment'] = environment\n",
    "    report_dict['country'] = country\n",
    "    report_dict['province'] = province\n",
    "    report_dict['location_details'] = location_details\n",
    "    \n",
    "    return report_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "id": "e2677faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For all sighting report urls, grab html and retrieve data, put into a dictionary\n",
    "# and append to a list of all results \n",
    "# this takes a while \n",
    "report_data = []\n",
    "for url in report_urls:\n",
    "    report_data.append(scrape_report_data(url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "id": "206f15ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5345"
      ]
     },
     "execution_count": 427,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(report_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "id": "ec93d609",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(report_data)\n",
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "id": "00c77187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write this raw data to a csv\n",
    "df.to_csv('data/bfro_raw.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c321b0b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bfro-env",
   "language": "python",
   "name": "bfro-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
